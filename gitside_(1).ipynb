{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adharv49/project/blob/main/gitside_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n_eu6qpBH0W",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/adharv49/project.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bz93Abt3BM0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForInpainting, AutoencoderKL\n",
        "from diffusers.utils import load_image\n",
        "import torch"
      ],
      "metadata": {
        "id": "547gwTtZBNXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipeline = AutoPipelineForInpainting.from_pretrained(\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\", vae=vae, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True).to(\"cuda\")\n",
        "pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\", low_cpu_mem_usage=True)"
      ],
      "metadata": {
        "id": "VutXRzkDBSqi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations\n",
        "!pip install iglovikov-helper-functions\n",
        "!pip install cloths_segmentation\n",
        "!pip install onnxruntime\n",
        "!pip install gradio\n",
        "!pip install insightface"
      ],
      "metadata": {
        "id": "0MqEIiOKBUsi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project"
      ],
      "metadata": {
        "id": "9IWf_2XyIbBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from SegBody import segment_body\n",
        "from diffusers.utils import load_image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import albumentations as albu\n",
        "from PIL import Image\n",
        "from iglovikov_helper_functions.utils.image_utils import load_rgb, pad, unpad\n",
        "from iglovikov_helper_functions.dl.pytorch.utils import tensor_from_rgb_image\n",
        "#from cloths_segmentation.pre_trained_models import create_model\n",
        "from diffusers import AutoencoderKL,AutoPipelineForInpainting\n",
        "\n",
        "def generate_masked_image(cloth_path, model_path):\n",
        "    cloth_image = load_image(cloth_path).convert(\"RGB\")\n",
        "    cloth_image.resize((512, 512))\n",
        "    model_image = load_image(model_path).convert(\"RGB\")\n",
        "    model_image.resize((512, 512))\n",
        "    seg_image, mask_image = segment_body(model_path, face=False)\n",
        "    mask_image.resize((512, 512))\n",
        "    pipeline.set_ip_adapter_scale(1.0)\n",
        "    images = pipeline(\n",
        "    prompt=\"photorealistic, perfect body, beautiful skin, realistic skin, natural skin\",\n",
        "    negative_prompt=\"ugly, bad quality, bad anatomy, deformed body, deformed hands, deformed feet,wrong alphabets, deformed face, deformed clothing, deformed skin, bad skin, leggings, tights, stockings\",\n",
        "    image=model_image,\n",
        "    mask_image=mask_image,\n",
        "    ip_adapter_image=cloth_image,\n",
        "    strength=0.99,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=100,\n",
        "    ).images\n",
        "    return images[0]"
      ],
      "metadata": {
        "id": "DmS7sW9LBcDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import os\n",
        "def run(cloth, model):\n",
        "    # Load the selected cloth and the uploaded model image\n",
        "    #cloth = Image.open(cloth_path)\n",
        "    model.save(\"temp_model.jpg\")\n",
        "    cloth.save(\"temp_cloth.jpg\")\n",
        "\n",
        "    # Call the masking function from mask.py\n",
        "    # Replace generate_masked_image with your actual function\n",
        "    masked_image = generate_masked_image(\"temp_cloth.jpg\", \"temp_model.jpg\")\n",
        "\n",
        "    # Return the masked image for display\n",
        "    return masked_image\n",
        "\n",
        "# Predefined cloth images\n",
        "CLOTH_OPTIONS = {\n",
        "    \"Cloth 1\": \"/content/project/samplecloth/01_upper.jpg\",\n",
        "    \"Cloth 2\": \"/content/project/samplecloth/06_upper.png\",\n",
        "    \"Cloth 3\": \"/content/project/samplecloth/09163_00.jpg\",\n",
        "    \"Cloth 4\": \"/content/project/samplecloth/09176_00.jpg\",\n",
        "    \"Cloth 5\": \"/content/project/samplecloth/09236_00.jpg\",\n",
        "    \"Cloth 6\": \"/content/project/samplecloth/09290_00.jpg\",\n",
        "    \"Cloth 7\": \"/content/project/samplecloth/boys-Puffer-Coat-With-Detachable-Hood-1.jpg\",\n",
        "    \"Cloth 8\": \"/content/project/samplecloth/brown-jacket.jpg\",\n",
        "    \"Cloth 9\": \"/content/project/samplecloth/clothing1.jpg\",\n",
        "    \"Cloth 10\": \"/content/project/samplecloth/puma-hoodie.jpg\",\n",
        "    \"Cloth 24\": \"/content/project/samplecloth/black-hoodie.jpg\"\n",
        "\n",
        "}\n",
        "\n",
        "# Gradio app\n",
        "with gr.Blocks() as app:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"<center><h1> DIGIDRAPE </h1><center>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Dropdown to select predefined cloths\n",
        "\n",
        "            cloth_input=gr.Image(sources='clipboard', type=\"pil\", label=\"Model\", value=CLOTH_OPTIONS[\"Cloth 1\"])\n",
        "            example = gr.Examples(inputs=cloth_input,\n",
        "                                  examples_per_page=4,\n",
        "                                  examples=[CLOTH_OPTIONS[\"Cloth 1\"], CLOTH_OPTIONS[\"Cloth 2\"],CLOTH_OPTIONS[\"Cloth 3\"],\n",
        "                                            CLOTH_OPTIONS[\"Cloth 4\"],CLOTH_OPTIONS[\"Cloth 5\"],CLOTH_OPTIONS[\"Cloth 6\"],\n",
        "                                            CLOTH_OPTIONS[\"Cloth 7\"],CLOTH_OPTIONS[\"Cloth 8\"],CLOTH_OPTIONS[\"Cloth 9\"],\n",
        "                                            CLOTH_OPTIONS[\"Cloth 10\"],CLOTH_OPTIONS[\"Cloth 24\"]])\n",
        "        with gr.Column():\n",
        "            model_input = gr.Image(sources=['upload'], type=\"pil\", label=\"Upload the Human Image\")\n",
        "\n",
        "    with gr.Row():\n",
        "        final_output = gr.Image(type=\"pil\", label=\"Final Prediction\")\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "\n",
        "\n",
        "    submit_button.click(fn=run, inputs=[cloth_input, model_input], outputs=[final_output])\n",
        "\n",
        "app.launch(debug=True, share=True)\n"
      ],
      "metadata": {
        "id": "4LzeOjw2Bdha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import os\n",
        "CLOTH_OPTIONS = {\n",
        "    \"Cloth 101\": \"/content/project/samplecloth/01_upper.jpg\",\n",
        "    \"Cloth 102\": \"/content/project/samplecloth/tantra.jpg\",\n",
        "    \"Cloth 103\": \"/content/project/samplecloth/09163_00.jpg\",\n",
        "    \"Cloth 104\": \"/content/project/samplecloth/09176_00.jpg\",\n",
        "    \"Cloth 105\": \"/content/project/samplecloth/09236_00.jpg\",\n",
        "    \"Cloth 106\": \"/content/project/samplecloth/09290_00.jpg\",\n",
        "     \"Cloth 107\": \"/content/project/samplecloth/clothing1.jpg\",\n",
        "    \"Cloth 108\": \"/content/project/samplecloth/brown-jacket.jpg\",\n",
        "    \"Cloth 109\": \"/content/project/samplecloth/boys-Puffer-Coat-With-Detachable-Hood-1.jpg\",\n",
        "    \"Cloth 110\": \"/content/project/samplecloth/boys-Puffer-Coat-With-Detachable-Hood-2.jpg\",\n",
        "   \"Cloth 111\":\"/content/g-polos-tshirt-2.png\",\n",
        "    \"Cloth 112\":\"/content/project/samplecloth/tantra.jpg\",\n",
        "    \"Cloth 113\": \"/content/pink jacket.jpg\",\n",
        "    \"Cloth 115\":\"/content/Man-Geox-Winter-jacket-1.jpg\",\n",
        "    \"Cloth 116\": \"/content/project/samplecloth/black-hoodie.jpg\",\n",
        "\n",
        "}\n",
        "def convert_png_to_jpg(input_path, output_path):\n",
        "    try:\n",
        "        # Open the PNG image\n",
        "        if isinstance(input_path, str):\n",
        "            # Open the PNG image if input_path is a file path\n",
        "            image = Image.open(input_path)\n",
        "        elif isinstance(input_path, Image.Image):\n",
        "            # Use the existing Image object\n",
        "            image = input_path\n",
        "        #image = Image.open(input_path)\n",
        "\n",
        "        # Convert RGBA (if present) to RGB\n",
        "        if image.mode == 'RGBA':\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        # Save as JPG\n",
        "        image.save(output_path, format=\"JPEG\")\n",
        "    finally:\n",
        "        print(\"Conversion completed.\")\n",
        "def run(request: gr.Request,cloth_key, model):\n",
        "    query_params = request.query_params\n",
        "    cloth_key = query_params.get(\"cloth\", \"Cloth 1\")\n",
        "    image_path =CLOTH_OPTIONS.get(cloth_key, None)\n",
        "    cloth= Image.open(image_path)\n",
        "    '''if not cloth_image:\n",
        "        raise ValueError(f\"Invalid cloth key: {cloth_key}\")'''\n",
        "\n",
        "    # Load the cloth image\n",
        "    #cloth = Image.open(cloth_path)\n",
        "    # Load the selected cloth and the uploaded model image\n",
        "    #cloth = Image.open(cloth_path)\n",
        "    model.save(\"temp_model.jpg\")\n",
        "    #cloth.save(\"temp_cloth.jpg\")\n",
        "    output_file = \"temp_cloth2.jpg\"  # Desired output file path\n",
        "    convert_png_to_jpg(input_path=cloth, output_path=output_file)\n",
        "    # Call the masking function from mask.py\n",
        "    # Replace generate_masked_image with your actual function\n",
        "    masked_image = generate_masked_image(\"temp_cloth2.jpg\", \"temp_model.jpg\")\n",
        "\n",
        "    # Return the masked image for display\n",
        "    return cloth,masked_image\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# DigiDrape Virtual Try-On System\")\n",
        "\n",
        "    # Input for cloth key\n",
        "    cloth_key_input = gr.Textbox(label=\"Cloth Key\", value=None, visible=False)\n",
        "    image_path = CLOTH_OPTIONS.get(cloth_key_input, None)\n",
        "    cloth_image = gr.Image(value=image_path, visible=False)\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        # Image display for cloth\n",
        "        cloth_image_output = gr.Image(label=\"Selected Cloth\")\n",
        "\n",
        "        # Upload human image\n",
        "        human_image_input = gr.Image(label=\"Upload Human Image\", type=\"pil\")\n",
        "        # Combined result display\n",
        "        combined_image_output = gr.Image(label=\"Combined Result\")\n",
        "\n",
        "    # Button to trigger processing\n",
        "    submit_button = gr.Button(\"Combine and Display\")\n",
        "\n",
        "    # Set up the function for the interface\n",
        "    submit_button.click(\n",
        "        run,\n",
        "        inputs=[cloth_key_input, human_image_input],\n",
        "        outputs=[cloth_image_output, combined_image_output],\n",
        "    )\n",
        "\n",
        "demo.launch(share=True,debug=True)\n"
      ],
      "metadata": {
        "id": "-C0zsAC_iQVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}